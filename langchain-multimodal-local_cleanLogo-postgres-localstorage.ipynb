{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install poppler tesseract libmagic\n",
    "#install globally\n",
    "#brew install tesseract poppler libmagic\n",
    "# echo 'export PATH=\"/opt/homebrew/bin:$PATH\"' >> ~/.zshrc\n",
    "# source ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bcbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Table, CompositeElement\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055aa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af24ad",
   "metadata": {},
   "source": [
    "### Setup langfuse for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langfuse\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\")\n",
    "\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=LANGFUSE_SECRET_KEY,\n",
    "    secret_key=LANGFUSE_PUBLIC_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# chain.invoke({\"input\": \"<user_input>\"}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ee977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if tesseract is accessible\n",
    "try:\n",
    "    result = subprocess.run([\"tesseract\", \"--version\"], capture_output=True, text=True)\n",
    "    print(\"Tesseract version:\", result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"Tesseract not found in PATH\")\n",
    "\n",
    "# Check PATH\n",
    "import os\n",
    "\n",
    "print(\"Current PATH:\", os.environ.get(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eaa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Your original settings\n",
    "content_folder = \"./content3/\"\n",
    "\n",
    "# Get all PDF files in content folder\n",
    "pdf_files = [f for f in os.listdir(content_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "chunks = []\n",
    "# Process each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(content_folder, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "\n",
    "    try:\n",
    "        # Your original chunking code\n",
    "        each_chunks = partition_pdf(\n",
    "            filename=file_path,\n",
    "            infer_table_structure=True,\n",
    "            include_page_breaks=True,\n",
    "            strategy=\"hi_res\",\n",
    "            extract_image_block_types=[\"Image\"],\n",
    "            extract_image_block_to_payload=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=10000,\n",
    "            combine_text_under_n_chars=2000,\n",
    "            new_after_n_chars=6000,\n",
    "        )\n",
    "\n",
    "        # Add filename to metadata for each chunk\n",
    "        for chunk in each_chunks:\n",
    "            chunk.metadata.filename = pdf_file  # Add filename here\n",
    "\n",
    "        chunks.extend(each_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9766bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Extract Content ===\n",
    "tables, texts = [], []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if isinstance(chunk, Table):\n",
    "        tables.append(chunk)\n",
    "    elif isinstance(chunk, CompositeElement):\n",
    "        texts.append(chunk)\n",
    "        for el in getattr(chunk.metadata, \"orig_elements\", []):\n",
    "            if isinstance(el, Table):\n",
    "                # Also add filename to nested tables\n",
    "                el.metadata.filename = chunk.metadata.filename\n",
    "                tables.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74415c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82754aee",
   "metadata": {},
   "source": [
    "### try to filter away logo images - using OCR and match the OCR text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import base64\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_likely_logo(image_base64):\n",
    "    \"\"\"\n",
    "    Check if image is likely a logo based on OCR text detection and size\n",
    "\n",
    "    Args:\n",
    "        image_base64: Base64 encoded image string\n",
    "        logo_keywords: List of keywords that indicate a logo\n",
    "        size_threshold: Tuple of (width, height) - images smaller than this are likely logos\n",
    "\n",
    "    Returns:\n",
    "        bool: True if likely a logo, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Customize these keywords based on your company logo text\n",
    "    logo_keywords = [\n",
    "        \"logo\",\n",
    "        \"company\",\n",
    "        \"inc\",\n",
    "        \"ltd\",\n",
    "        \"corp\",\n",
    "        \"llc\",\n",
    "        \"trademark\",\n",
    "        \"®\",\n",
    "        \"©\",\n",
    "        \"copyright\",\n",
    "        \"MINISTRY OF MANPOWER\",\n",
    "        \"MINISTRY OF\",\n",
    "        \"ACCENTURE\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        print(f\"==>> image: {image}\")\n",
    "\n",
    "        # Check image size first (quick filter)\n",
    "        width, height = image.size\n",
    "        print(f\"==>> height: {height}\")\n",
    "        print(f\"==>> width: {width}\")\n",
    "        # if width < size_threshold[0] and height < size_threshold[1]:\n",
    "        #     return True  # Small images are likely logos\n",
    "\n",
    "        # Convert to grayscale for better OCR\n",
    "        if image.mode != \"L\":\n",
    "            image = image.convert(\"L\")\n",
    "\n",
    "        # Enhance image for better OCR (optional)\n",
    "        # Convert PIL to numpy array for OpenCV processing\n",
    "        img_array = np.array(image)\n",
    "        print(f\"==>> img_array: {img_array}\")\n",
    "\n",
    "        # Apply some preprocessing to improve OCR accuracy\n",
    "        # Increase contrast\n",
    "        img_array = cv2.convertScaleAbs(img_array, alpha=1.5, beta=0)\n",
    "        print(f\"==>> img_array: {img_array}\")\n",
    "\n",
    "        # Convert back to PIL\n",
    "        enhanced_image = Image.fromarray(img_array)\n",
    "        print(f\"==>> enhanced_image: {enhanced_image}\")\n",
    "\n",
    "        # Extract text using OCR\n",
    "        text = (\n",
    "            pytesseract.image_to_string(enhanced_image, config=\"--psm 6\")\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        print(f\"==>> OCR text: '{text}'\")\n",
    "        print(f\"==>> Logo keywords: {logo_keywords}\")\n",
    "\n",
    "        # # Check for logo keywords\n",
    "        for keyword in logo_keywords:\n",
    "            keyword_lower = keyword.lower()\n",
    "            if keyword_lower in text:\n",
    "                print(f\"==>> MATCH FOUND: '{keyword_lower}' in '{text}' - FILTERING OUT\")\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        # If we can't process the image, keep it to be safe\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d333e5f",
   "metadata": {},
   "source": [
    "### old version just return base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_images_base64_filtered(chunks, filter_logos=True):\n",
    "#     \"\"\"\n",
    "#     Extract images from chunks with optional logo filtering\n",
    "\n",
    "#     Args:\n",
    "#         chunks: List of chunks from partition_pdf\n",
    "#         filter_logos: Whether to filter out logos\n",
    "#         logo_keywords: List of keywords that indicate a logo\n",
    "#         size_threshold: Tuple of (width, height) for size-based filtering\n",
    "\n",
    "#     Returns:\n",
    "#         List of base64 encoded images (logos filtered out if enabled)\n",
    "#     \"\"\"\n",
    "#     images_b64 = []\n",
    "#     filtered_count = 0\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         if \"CompositeElement\" in str(type(chunk)):\n",
    "#             chunk_els = chunk.metadata.orig_elements\n",
    "#             for el in chunk_els:\n",
    "#                 if \"Image\" in str(type(el)):\n",
    "#                     image_base64 = el.metadata.image_base64\n",
    "\n",
    "#                     if filter_logos:\n",
    "#                         if is_likely_logo(image_base64):\n",
    "#                             filtered_count += 1\n",
    "#                             print(\n",
    "#                                 f\"Filtered out likely logo image (total filtered: {filtered_count})\"\n",
    "#                             )\n",
    "#                             continue\n",
    "\n",
    "#                     images_b64.append(image_base64)\n",
    "\n",
    "#     print(\n",
    "#         f\"Total images extracted: {len(images_b64)}, Logos filtered: {filtered_count}\"\n",
    "#     )\n",
    "#     return images_b64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b338a5d",
   "metadata": {},
   "source": [
    "### new version return filename and base64 for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_base64_filtered(chunks, filter_logos=True):\n",
    "    \"\"\"\n",
    "    Extract images from chunks with optional logo filtering\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunks from partition_pdf\n",
    "        filter_logos: Whether to filter out logos\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (images_b64, image_filenames)\n",
    "    \"\"\"\n",
    "    images_b64 = []\n",
    "    image_filenames = []\n",
    "    filtered_count = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    image_base64 = el.metadata.image_base64\n",
    "\n",
    "                    if filter_logos:\n",
    "                        if is_likely_logo(image_base64):\n",
    "                            filtered_count += 1\n",
    "                            print(\n",
    "                                f\"Filtered out likely logo image (total filtered: {filtered_count})\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                    images_b64.append(image_base64)\n",
    "                    # Get filename from chunk metadata\n",
    "                    source_filename = getattr(chunk.metadata, \"filename\", \"unknown.pdf\")\n",
    "                    image_filenames.append(source_filename)\n",
    "\n",
    "    print(\n",
    "        f\"Total images extracted: {len(images_b64)}, Logos filtered: {filtered_count}\"\n",
    "    )\n",
    "    return images_b64, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3d937",
   "metadata": {},
   "source": [
    "### old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = get_images_base64_filtered(\n",
    "#     chunks,\n",
    "#     filter_logos=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36d4ce",
   "metadata": {},
   "source": [
    "### new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_source_filenames = get_images_base64_filtered(\n",
    "    chunks,\n",
    "    filter_logos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d303d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM for Text + Table Summarization ===\n",
    "text_model = ChatOllama(model=\"llama3:8b\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8477d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "Respond only with the summary and do not start with any introduction like here is the concise summary.\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "text_prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = (\n",
    "    {\"element\": lambda x: x} | text_prompt | text_model | StrOutputParser()\n",
    ")\n",
    "\n",
    "text_summaries = summarize_chain.batch(texts)\n",
    "table_summaries = summarize_chain.batch([t.metadata.text_as_html for t in tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISION_MODEL = \"gemma3:12b\"  # For image analysis (alternatives: llava:7b, bakllava)\n",
    "vision_model = ChatOllama(\n",
    "    model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1cf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_with_ollama(image_base64: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze image using Ollama vision model\n",
    "    Note: This approach works with models like llava that support vision\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"Describe this image in detail. For context, \n",
    "    the image is part of a Singapore Ministry of Manpower workpass system. Be specific about images, such as, diagrams, flowchart, screenshot and any text visible in the image. Do not respond with any introduction words like Here\\'s a detailed description of the image. \"\"\"\n",
    "\n",
    "    # Create message with image\n",
    "    messages = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": prompt_template},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = vision_model.invoke(messages)\n",
    "        print(f\"==>> response: {response}\")\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing image: {e}\")\n",
    "        return f\"Error analyzing image: Unable to process with {VISION_MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries = []\n",
    "for i, img_b64 in enumerate(images):\n",
    "    print(f\"Processing image {i+1}/{len(images)}\")\n",
    "    summary = analyze_image_with_ollama(img_b64)\n",
    "    image_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee01acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74262ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_postgres\n",
    "!pip install psycopg_binary\n",
    "from utils.store import PostgresByteStore\n",
    "from database import COLLECTION_NAME, CONNECTION_STRING\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"multi_modal_rag_ollama\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=\"./chroma_db_8\",  # Separate directory for Ollama version\n",
    "# )\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "# Storage setup (unchanged)\n",
    "# store = InMemoryStore()\n",
    "store = PostgresByteStore(CONNECTION_STRING, COLLECTION_NAME)\n",
    "# store = LocalFileStore(\"./document_store_ollama\")  # Alternative persistent storage\n",
    "id_key = \"doc_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c60cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiVector Retriever setup (unchanged)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b551b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd15471",
   "metadata": {},
   "source": [
    "### Define filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_text = []\n",
    "for i, text in enumerate(texts):\n",
    "    if hasattr(text, \"metadata\") and hasattr(text.metadata, \"filename\"):\n",
    "        filename = text.metadata.filename\n",
    "    elif (\n",
    "        hasattr(text, \"metadata\")\n",
    "        and isinstance(text.metadata, dict)\n",
    "        and \"filename\" in text.metadata\n",
    "    ):\n",
    "        filename = text.metadata[\"filename\"]\n",
    "    else:\n",
    "        filename = \"NO_FILENAME\"\n",
    "\n",
    "    print(filename)\n",
    "    filenames_for_text.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding texts to retriever...\")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]})\n",
    "    for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts, filenames_for_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70382870",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd30e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_tables = []\n",
    "for i, text in enumerate(tables):\n",
    "    if hasattr(text, \"metadata\") and hasattr(text.metadata, \"filename\"):\n",
    "        filename = text.metadata.filename\n",
    "    elif (\n",
    "        hasattr(text, \"metadata\")\n",
    "        and isinstance(text.metadata, dict)\n",
    "        and \"filename\" in text.metadata\n",
    "    ):\n",
    "        filename = text.metadata[\"filename\"]\n",
    "    else:\n",
    "        filename = \"NO_FILENAME\"\n",
    "\n",
    "    print(filename)\n",
    "    filenames_for_tables.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding tables to retriever...\")\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]})\n",
    "    for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01131ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(list(zip(table_ids, tables, filenames_for_tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_images = []\n",
    "for i, source_filename in enumerate(image_source_filenames):\n",
    "    # Create unique filename for each image\n",
    "    image_filename = f\"{source_filename}_image_{i}\"\n",
    "    filenames_for_images.append(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a347f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437430c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding images to retriever...\")\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]})\n",
    "    for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56976c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(list(zip(img_ids, images, filenames_for_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1a87f",
   "metadata": {},
   "source": [
    "### Check In memory store data - 15 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the keys currently in the store\n",
    "all_doc_ids = store.yield_keys()\n",
    "\n",
    "# Loop through and fetch each document by its ID\n",
    "for doc_id in all_doc_ids:\n",
    "    docs = store.mget([doc_id])  # Returns a list with the document(s)\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    for doc in docs:\n",
    "        print(doc)  # `doc` is a Document object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f1f61",
   "metadata": {},
   "source": [
    "### check Chroma document - 15 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59dee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs = vectorstore.get()\n",
    "\n",
    "# index = 0\n",
    "# for doc in all_docs[\"documents\"]:\n",
    "#     print(\"index is :\", index)\n",
    "#     print(doc)\n",
    "#     index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96abdf6",
   "metadata": {},
   "source": [
    "### list top 1000 records in postgres vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(all_docs)} documents\")\n",
    "for doc in all_docs:\n",
    "    print(f\"ID: {doc.metadata.get('doc_id')}, Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-modal RAG setup complete!\")\n",
    "print(f\"Processed: {len(texts)} texts, {len(tables)} tables, {len(images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58b28f",
   "metadata": {},
   "source": [
    "### Not required search_kwargs unless, need to specifically retrieve top few results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.search_kwargs = {\"k\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"How to stop automatic FIN generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14cd56",
   "metadata": {},
   "source": [
    "### some sameples using similarity_search can get the doc id but .invoke cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the retriever is actually doing\n",
    "docs = retriever.invoke(\"How to stop automatic FIN generation\")\n",
    "\n",
    "# The retriever first searches vector store for summaries\n",
    "query = \"How to stop automatic FIN generation\"\n",
    "relevant_summaries = retriever.vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "print(\"=== RELEVANT SUMMARIES FROM VECTOR STORE ===\")\n",
    "for summary in relevant_summaries:\n",
    "    print(f\"Summary: {summary.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {summary.metadata}\")\n",
    "    doc_id = summary.metadata.get(\"doc_id\")\n",
    "    if doc_id:\n",
    "        print(f\"Will retrieve full doc with ID: {doc_id}\")\n",
    "    print()\n",
    "\n",
    "# Then retrieves full docs from byte store\n",
    "if relevant_summaries:\n",
    "    doc_ids = [\n",
    "        summary.metadata[\"doc_id\"]\n",
    "        for summary in relevant_summaries\n",
    "        if \"doc_id\" in summary.metadata\n",
    "    ]\n",
    "    print(f\"Looking up doc_ids: {doc_ids}\")\n",
    "    full_docs = retriever.docstore.mget(doc_ids)\n",
    "    print(f\"Retrieved {len(full_docs)} full documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f276f",
   "metadata": {},
   "source": [
    "### Print the formatted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c29211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from base64 import b64decode\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Choose your preferred model\n",
    "RAG_MODEL = (\n",
    "    \"llama3.1:8b\"  # Recommended alternatives: \"llama3.2:3b\", \"mistral:7b\", \"qwen2:7b\"\n",
    ")\n",
    "VISION_MODEL = \"gemma3:12b\"  # For handling images in RAG\n",
    "\n",
    "print(f\"Using RAG model: {RAG_MODEL}\")\n",
    "print(f\"Using Vision model: {VISION_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    \"\"\"\n",
    "    Split retrieved documents into base64-encoded images and text content\n",
    "\n",
    "    Args:\n",
    "        docs: List of retrieved documents from the vector store\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains separated 'images' and 'texts' lists\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(docs)} retrieved documents\")\n",
    "\n",
    "    b64_images = []\n",
    "    text_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        print(f\"==>> doc: {doc}\")\n",
    "        # Check if document content is base64 encoded (likely an image)\n",
    "        try:\n",
    "            # Try to decode as base64\n",
    "            # decoded = b64decode(doc.page_content)\n",
    "            b64decode(doc)\n",
    "            # If successful, it's likely base64 encoded image data\n",
    "            b64_images.append(doc)\n",
    "            print(f\"Found base64 image document\")\n",
    "        except Exception as e:\n",
    "            # If decoding fails, treat as text\n",
    "            text_docs.append(doc)\n",
    "            print(f\"Found text document: {doc}...\")\n",
    "\n",
    "    return {\"images\": b64_images, \"texts\": text_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5638d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_text_only(kwargs):\n",
    "    \"\"\"\n",
    "    Build prompt for text-only RAG (when no images are present)\n",
    "    Uses the main RAG model for faster processing\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    # Combine all text content\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_doc in docs_by_type[\"texts\"]:\n",
    "            # Handle both string content and Document objects\n",
    "            if hasattr(text_doc, \"page_content\"):\n",
    "                context_text += text_doc.page_content + \"\\n\\n\"\n",
    "            else:\n",
    "                context_text += str(text_doc) + \"\\n\\n\"\n",
    "\n",
    "    # Simple text-based prompt template\n",
    "    prompt_template = f\"\"\"You are a helpful assistant answering questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context_text.strip()}\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- If the context doesn't contain relevant information, say \"I don't have enough information to answer this question based on the provided context\"\n",
    "- Be concise and accurate\n",
    "- If referencing specific data or facts, mention them clearly\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_vision(kwargs):\n",
    "    \"\"\"\n",
    "    Build prompt for multi-modal RAG (when images are present)\n",
    "    Uses the vision model to handle both text and images\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    # Combine text content\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_doc in docs_by_type[\"texts\"]:\n",
    "            if hasattr(text_doc, \"page_content\"):\n",
    "                context_text += text_doc.page_content + \"\\n\\n\"\n",
    "            else:\n",
    "                context_text += str(text_doc) + \"\\n\\n\"\n",
    "\n",
    "    # Base prompt text\n",
    "    prompt_text = f\"\"\"You are a helpful assistant answering questions based on the provided context, which includes both text and images.\n",
    "\n",
    "Text Context:\n",
    "{context_text.strip()}\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based on both the text context and the images provided\n",
    "- If analyzing images, describe what you see that's relevant to the question\n",
    "- Be specific about information from images (charts, diagrams, etc.)\n",
    "- If the context doesn't contain relevant information, say so clearly\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Build content list starting with text\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "    # Add images if present\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        print(f\"Adding {len(docs_by_type['images'])} images to prompt\")\n",
    "        for i, image_b64 in enumerate(docs_by_type[\"images\"]):\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([HumanMessage(content=prompt_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_and_prompt(kwargs):\n",
    "    \"\"\"\n",
    "    Dynamically choose between text-only and vision model based on content\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    print(f\"==>> docs_by_type: {docs_by_type}\")\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        # Use vision model for multi-modal content\n",
    "        print(\"Using vision model for multi-modal RAG\")\n",
    "        prompt = build_prompt_with_vision(kwargs)\n",
    "        model = ChatOllama(\n",
    "            model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "    else:\n",
    "        # Use text model for text-only content (faster)\n",
    "        print(\"Using text model for text-only RAG\")\n",
    "        prompt = build_prompt_text_only(kwargs)\n",
    "        model = ChatOllama(\n",
    "            model=RAG_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "\n",
    "    return prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ae489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG Chain\n",
    "print(\"Setting up RAG chain...\")\n",
    "chain = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(choose_model_and_prompt)\n",
    "\n",
    "# Alternative: Simple chain that always uses text model (faster but no vision)\n",
    "simple_text_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt_text_only)\n",
    "    | ChatOllama(model=RAG_MODEL, temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain with sources (returns both context and response)\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(response=RunnableLambda(choose_model_and_prompt))\n",
    "\n",
    "# Alternative: Always use vision model (slower but handles all content types)\n",
    "vision_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt_with_vision)\n",
    "    | ChatOllama(model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "vision_chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt_with_vision)\n",
    "        | ChatOllama(\n",
    "            model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langfuse\n",
    "import langfuse\n",
    "print('Hello',langfuse.__version__)\n",
    "\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"topic\": \"cats\"}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d91721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question\n",
    "response = simple_text_chain.invoke(\n",
    "    \"How is fin created?\", config={\"callbacks\": [langfuse_handler]}\n",
    ")\n",
    "print(f\"==>> response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0374d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question\n",
    "response = chain.invoke(\"When will the wpno for Thailand used up?\", config={\"callbacks\": [langfuse_handler]}\n",
    ")\n",
    "print(f\"==>> response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sources\n",
    "response = chain_with_sources.invoke(\"How was fin generated\")\n",
    "print(f\"==>> response: {response}\")\n",
    "print(\"Response:\", response[\"response\"])\n",
    "print(\"Context used:\", response[\"context\"])\n",
    "for text in response[\"context\"][\"texts\"]:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "for image in response[\"context\"][\"images\"]:\n",
    "    display_base64_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
