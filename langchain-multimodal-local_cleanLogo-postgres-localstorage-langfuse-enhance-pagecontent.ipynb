{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install poppler tesseract libmagic\n",
    "#install globally\n",
    "#brew install tesseract poppler libmagic\n",
    "# echo 'export PATH=\"/opt/homebrew/bin:$PATH\"' >> ~/.zshrc\n",
    "# source ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3bcbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "#Langchain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import Table, CompositeElement\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "#Langfuse\n",
    "!pip install langfuse\n",
    "from langfuse import Langfuse\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17af24ad",
   "metadata": {},
   "source": [
    "### Setup langfuse for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ecb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('os.environ.get(\"LANGFUSE_PUBLIC_KEY\")', os.environ.get(\"LANGFUSE_PUBLIC_KEY\"))\n",
    "print('os.environ.get(\"LANGFUSE_SECRET_KEY\")', os.environ.get(\"LANGFUSE_SECRET_KEY\"))\n",
    "print('os.environ.get(\"LANGFUSE_HOST\")', os.environ.get(\"LANGFUSE_HOST\"))\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.environ.get(\"LANGFUSE_HOST\"),\n",
    ")\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a4c1e",
   "metadata": {},
   "source": [
    "### Simple test makesure langfuse is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059376aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Create a simple chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful AI assistant. Answer questions clearly and concisely.\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "# Test with one question\n",
    "question = \"Is job market bad currently?\"\n",
    "\n",
    "print(\"Testing ChatOllama with Langfuse tracing...\")\n",
    "print(f\"Question: {question}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    # Invoke the chain with Langfuse callback\n",
    "    response = chain.invoke(\n",
    "        {\"question\": question}, config={\"callbacks\": [langfuse_handler]}\n",
    "    )\n",
    "\n",
    "    print(f\"Response: {response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure Ollama is running and llama3.1:8b is available\")\n",
    "\n",
    "print(\"\\nCheck your Langfuse dashboard at http://localhost:3000 to see the trace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfeb0fe",
   "metadata": {},
   "source": [
    "## setup some path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ee977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Check if tesseract is accessible\n",
    "try:\n",
    "    result = subprocess.run([\"tesseract\", \"--version\"], capture_output=True, text=True)\n",
    "    print(\"Tesseract version:\", result.stdout)\n",
    "except FileNotFoundError:\n",
    "    print(\"Tesseract not found in PATH\")\n",
    "\n",
    "# Check PATH\n",
    "import os\n",
    "\n",
    "print(\"Current PATH:\", os.environ.get(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebcee4",
   "metadata": {},
   "source": [
    "### Chunk PDF by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eaa67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Your original settings\n",
    "content_folder = \"./content2/\"\n",
    "\n",
    "# Get all PDF files in content folder\n",
    "pdf_files = [f for f in os.listdir(content_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "chunks = []\n",
    "# Process each PDF file\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(content_folder, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "\n",
    "    try:\n",
    "        # Your original chunking code\n",
    "        each_chunks = partition_pdf(\n",
    "            filename=file_path,\n",
    "            infer_table_structure=True,\n",
    "            include_page_breaks=True,\n",
    "            strategy=\"hi_res\",\n",
    "            extract_image_block_types=[\"Image\"],\n",
    "            extract_image_block_to_payload=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=10000,\n",
    "            combine_text_under_n_chars=2000,\n",
    "            new_after_n_chars=6000,\n",
    "        )\n",
    "\n",
    "        # Add filename to metadata for each chunk\n",
    "        for chunk in each_chunks:\n",
    "            chunk.metadata.filename = pdf_file  # Add filename here\n",
    "\n",
    "        chunks.extend(each_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error processing {pdf_file}: {e}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300982d",
   "metadata": {},
   "source": [
    "### extract and differentiate tables and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import Table, CompositeElement\n",
    "\n",
    "# === Extract Content ===\n",
    "tables, texts = [], []\n",
    "\n",
    "count = 0\n",
    "for chunk in chunks:\n",
    "    count = count = 1\n",
    "    if isinstance(chunk, Table):   ## Actually this line like not use.\n",
    "        print(\"chunk\" + str(count))\n",
    "        tables.append(chunk)\n",
    "    elif isinstance(chunk, CompositeElement): \n",
    "        texts.append(chunk)\n",
    "        for el in getattr(chunk.metadata, \"orig_elements\", []):\n",
    "            if isinstance(el, Table):\n",
    "                print(\"chunk\" + str(count))\n",
    "                # Also add filename to nested tables\n",
    "                el.metadata.filename = chunk.metadata.filename\n",
    "                tables.append(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74415c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bdf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82754aee",
   "metadata": {},
   "source": [
    "### Prepare function to try to filter away logo images - using OCR and match the OCR text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytesseract\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import base64\n",
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_likely_logo(image_base64):\n",
    "    \"\"\"\n",
    "    Check if image is likely a logo based on OCR text detection and size\n",
    "\n",
    "    Args:\n",
    "        image_base64: Base64 encoded image string\n",
    "        logo_keywords: List of keywords that indicate a logo\n",
    "        size_threshold: Tuple of (width, height) - images smaller than this are likely logos\n",
    "\n",
    "    Returns:\n",
    "        bool: True if likely a logo, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Customize these keywords based on your company logo text\n",
    "    logo_keywords = [\n",
    "        \"logo\",\n",
    "        \"company\",\n",
    "        \"inc\",\n",
    "        \"ltd\",\n",
    "        \"corp\",\n",
    "        \"llc\",\n",
    "        \"trademark\",\n",
    "        \"Â®\",\n",
    "        \"Â©\",\n",
    "        \"copyright\",\n",
    "        \"MINISTRY OF MANPOWER\",\n",
    "        \"MINISTRY OF\",\n",
    "        \"ACCENTURE\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Decode base64 image\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        print(f\"==>> image: {image}\")\n",
    "\n",
    "        # Check image size first (quick filter)\n",
    "        width, height = image.size\n",
    "        print(f\"==>> height: {height}\")\n",
    "        print(f\"==>> width: {width}\")\n",
    "        # if width < size_threshold[0] and height < size_threshold[1]:\n",
    "        #     return True  # Small images are likely logos\n",
    "\n",
    "        # Convert to grayscale for better OCR\n",
    "        if image.mode != \"L\":\n",
    "            image = image.convert(\"L\")\n",
    "\n",
    "        # Enhance image for better OCR (optional)\n",
    "        # Convert PIL to numpy array for OpenCV processing\n",
    "        img_array = np.array(image)\n",
    "        print(f\"==>> img_array: {img_array}\")\n",
    "\n",
    "        # Apply some preprocessing to improve OCR accuracy\n",
    "        # Increase contrast\n",
    "        img_array = cv2.convertScaleAbs(img_array, alpha=1.5, beta=0)\n",
    "        print(f\"==>> img_array: {img_array}\")\n",
    "\n",
    "        # Convert back to PIL\n",
    "        enhanced_image = Image.fromarray(img_array)\n",
    "        print(f\"==>> enhanced_image: {enhanced_image}\")\n",
    "\n",
    "        # Extract text using OCR\n",
    "        text = (\n",
    "            pytesseract.image_to_string(enhanced_image, config=\"--psm 6\")\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        print(f\"==>> OCR text: '{text}'\")\n",
    "        print(f\"==>> Logo keywords: {logo_keywords}\")\n",
    "\n",
    "        # # Check for logo keywords\n",
    "        for keyword in logo_keywords:\n",
    "            keyword_lower = keyword.lower()\n",
    "            if keyword_lower in text:\n",
    "                print(f\"==>> MATCH FOUND: '{keyword_lower}' in '{text}' - FILTERING OUT\")\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        # If we can't process the image, keep it to be safe\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d333e5f",
   "metadata": {},
   "source": [
    "### old version just return base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_images_base64_filtered(chunks, filter_logos=True):\n",
    "#     \"\"\"\n",
    "#     Extract images from chunks with optional logo filtering\n",
    "\n",
    "#     Args:\n",
    "#         chunks: List of chunks from partition_pdf\n",
    "#         filter_logos: Whether to filter out logos\n",
    "#         logo_keywords: List of keywords that indicate a logo\n",
    "#         size_threshold: Tuple of (width, height) for size-based filtering\n",
    "\n",
    "#     Returns:\n",
    "#         List of base64 encoded images (logos filtered out if enabled)\n",
    "#     \"\"\"\n",
    "#     images_b64 = []\n",
    "#     filtered_count = 0\n",
    "\n",
    "#     for chunk in chunks:\n",
    "#         if \"CompositeElement\" in str(type(chunk)):\n",
    "#             chunk_els = chunk.metadata.orig_elements\n",
    "#             for el in chunk_els:\n",
    "#                 if \"Image\" in str(type(el)):\n",
    "#                     image_base64 = el.metadata.image_base64\n",
    "\n",
    "#                     if filter_logos:\n",
    "#                         if is_likely_logo(image_base64):\n",
    "#                             filtered_count += 1\n",
    "#                             print(\n",
    "#                                 f\"Filtered out likely logo image (total filtered: {filtered_count})\"\n",
    "#                             )\n",
    "#                             continue\n",
    "\n",
    "#                     images_b64.append(image_base64)\n",
    "\n",
    "#     print(\n",
    "#         f\"Total images extracted: {len(images_b64)}, Logos filtered: {filtered_count}\"\n",
    "#     )\n",
    "#     return images_b64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b338a5d",
   "metadata": {},
   "source": [
    "### new version return filename and base64 for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276eb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_base64_filtered(chunks, filter_logos=True):\n",
    "    \"\"\"\n",
    "    Extract images from chunks with optional logo filtering\n",
    "\n",
    "    Args:\n",
    "        chunks: List of chunks from partition_pdf\n",
    "        filter_logos: Whether to filter out logos\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (images_b64, image_filenames)\n",
    "    \"\"\"\n",
    "    images_b64 = []\n",
    "    image_filenames = []\n",
    "    filtered_count = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    image_base64 = el.metadata.image_base64\n",
    "\n",
    "                    if filter_logos:\n",
    "                        if is_likely_logo(image_base64):\n",
    "                            filtered_count += 1\n",
    "                            print(\n",
    "                                f\"Filtered out likely logo image (total filtered: {filtered_count})\"\n",
    "                            )\n",
    "                            continue\n",
    "\n",
    "                    images_b64.append(image_base64)\n",
    "                    # Get filename from chunk metadata\n",
    "                    source_filename = getattr(chunk.metadata, \"filename\", \"unknown.pdf\")\n",
    "                    image_filenames.append(source_filename)\n",
    "\n",
    "    print(\n",
    "        f\"Total images extracted: {len(images_b64)}, Logos filtered: {filtered_count}\"\n",
    "    )\n",
    "    return images_b64, image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3d937",
   "metadata": {},
   "source": [
    "### old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = get_images_base64_filtered(\n",
    "#     chunks,\n",
    "#     filter_logos=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36d4ce",
   "metadata": {},
   "source": [
    "### new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, image_source_filenames = get_images_base64_filtered(\n",
    "    chunks,\n",
    "    filter_logos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d303d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_source_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "# === LLM for Text + Table Summarization ===\n",
    "text_model = ChatOllama(model=\"llama3:8b\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce915ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb41cd1",
   "metadata": {},
   "source": [
    "### Old solution unable to see input trace in langfuse using {\"element\": lambda x: x}  langfuse confuse dont know what to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# langfuse_handler = CallbackHandler()\n",
    "\n",
    "# prompt_text = \"\"\"\n",
    "# You are an assistant tasked with summarizing tables and text.\n",
    "# Give a concise summary of the table or text.\n",
    "# Respond only with the summary and do not start with any introduction like here is the concise summary.\n",
    "# Table or text chunk: {text}\n",
    "# \"\"\"\n",
    "# text_prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "# summarize_chain = (\n",
    "#     {\"text\": lambda x: x} | text_prompt | text_model | StrOutputParser()\n",
    "# )\n",
    "\n",
    "\n",
    "# # Convert to proper input format\n",
    "# text_inputs = [{\"text\": text} for text in texts]\n",
    "# text_summaries = summarize_chain.batch(\n",
    "#     text_inputs, config={\"callbacks\": [langfuse_handler]}\n",
    "# )\n",
    "\n",
    "# text_summaries = summarize_chain.batch(texts, config={\"callbacks\": [langfuse_handler]})\n",
    "# table_summaries = summarize_chain.batch([t.metadata.text_as_html for t in tables])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d077c",
   "metadata": {},
   "source": [
    "### New way to write for summarising Text and Tables. Explicitly extract text upfront. This allow langfuse to detect the input, good for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e009a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "Respond only with the summary and do not start with any introduction like here is the concise summary.\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "text_prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = text_prompt | text_model | StrOutputParser()\n",
    "\n",
    "text_summaries = summarize_chain.batch(\n",
    "    [{\"element\": text.text} for text in texts], config={\"callbacks\": [langfuse_handler]}\n",
    ")\n",
    "\n",
    "table_summaries = summarize_chain.batch(\n",
    "    [{\"element\": t.metadata.text_as_html} for t in tables],\n",
    "    config={\"callbacks\": [langfuse_handler]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISION_MODEL = \"gemma3:12b\"  # For image analysis (alternatives: llava:7b, bakllava)\n",
    "vision_model = ChatOllama(\n",
    "    model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00174c",
   "metadata": {},
   "source": [
    "### Ask LLM to summarise image with base64 image text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1cf099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "def analyze_image_with_ollama(image_base64: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze image using Ollama vision model\n",
    "    Note: This approach works with models like llava that support vision\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"Describe this image in detail. For context, \n",
    "    the image is part of a Singapore Ministry of Manpower workpass system. Be specific about images, such as, diagrams, flowchart, screenshot and any text visible in the image. Do not respond with any introduction words like Here\\'s a detailed description of the image. \"\"\"\n",
    "\n",
    "    # Create message with image\n",
    "    messages = [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": prompt_template},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = vision_model.invoke(\n",
    "            messages, config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "        print(f\"==>> response: {response}\")\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing image: {e}\")\n",
    "        return f\"Error analyzing image: Unable to process with {VISION_MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries = []\n",
    "for i, img_b64 in enumerate(images):\n",
    "    print(f\"Processing image {i+1}/{len(images)}\")\n",
    "    summary = analyze_image_with_ollama(img_b64)\n",
    "    image_summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0907379",
   "metadata": {},
   "source": [
    "### Init embedding model for Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee01acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b92a2",
   "metadata": {},
   "source": [
    "### Init vector db, cuurently using PG vector and bytestore for localstore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74262ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_postgres\n",
    "!pip install psycopg_binary\n",
    "from utils.store import PostgresByteStore\n",
    "from database import COLLECTION_NAME, CONNECTION_STRING\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"multi_modal_rag_ollama\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=\"./chroma_db_8\",  # Separate directory for Ollama version\n",
    "# )\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\n",
    "# Storage setup (unchanged)\n",
    "# store = InMemoryStore()\n",
    "store = PostgresByteStore(CONNECTION_STRING, COLLECTION_NAME)\n",
    "# store = LocalFileStore(\"./document_store_ollama\")  # Alternative persistent storage\n",
    "id_key = \"doc_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c60cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b551b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd15471",
   "metadata": {},
   "source": [
    "### Define filename to map record for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_text = []\n",
    "for i, text in enumerate(texts):\n",
    "    if hasattr(text, \"metadata\") and hasattr(text.metadata, \"filename\"):\n",
    "        filename = text.metadata.filename\n",
    "    elif (\n",
    "        hasattr(text, \"metadata\")\n",
    "        and isinstance(text.metadata, dict)\n",
    "        and \"filename\" in text.metadata\n",
    "    ):\n",
    "        filename = text.metadata[\"filename\"]\n",
    "    else:\n",
    "        filename = \"NO_FILENAME\"\n",
    "\n",
    "    print(filename)\n",
    "    filenames_for_text.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72ef2f",
   "metadata": {},
   "source": [
    "### Old way - Add both vector(PGVector) and docstore to postgres for texts\n",
    "### Langfuse cant detect the structure since it is not a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uuid\n",
    "# from langchain_core.documents import Document\n",
    "\n",
    "# print(\"Adding texts to retriever...\")\n",
    "# doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "# summary_texts = [\n",
    "#     Document(page_content=summary, metadata={id_key: doc_ids[i]})\n",
    "#     for i, summary in enumerate(text_summaries)\n",
    "# ]\n",
    "# retriever.vectorstore.add_documents(summary_texts)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, texts, filenames_for_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a997fb",
   "metadata": {},
   "source": [
    "### New way using Document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding texts to retriever...\")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "\n",
    "# Create summary documents for vectorstore\n",
    "summary_texts = [\n",
    "    Document(\n",
    "        page_content=summary,\n",
    "        metadata={\n",
    "            id_key: doc_ids[i],\n",
    "            \"doc_type\": \"text\",\n",
    "            \"filename\": (\n",
    "                filenames_for_text[i] if i < len(filenames_for_text) else f\"text_{i}\"\n",
    "            ),\n",
    "            \"content_type\": \"text_summary\",\n",
    "        },\n",
    "    )\n",
    "    for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "\n",
    "# Create full documents for docstore\n",
    "full_text_docs = [\n",
    "    Document(\n",
    "        page_content=text if isinstance(text, str) else str(text),\n",
    "        metadata={\n",
    "            id_key: doc_ids[i],\n",
    "            \"doc_type\": \"text\",\n",
    "            \"filename\": (\n",
    "                filenames_for_text[i] if i < len(filenames_for_text) else f\"text_{i}\"\n",
    "            ),\n",
    "            \"content_type\": \"text_full\",\n",
    "        },\n",
    "    )\n",
    "    for i, text in enumerate(texts)\n",
    "]\n",
    "\n",
    "# Add summaries to vectorstore for searching\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "\n",
    "# Add full documents to docstore for retrieval - FIXED FORMAT\n",
    "retriever.docstore.mset(list(zip(doc_ids, full_text_docs, filenames_for_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70382870",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99676b71",
   "metadata": {},
   "source": [
    "### Define filename to map record for tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd30e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_tables = []\n",
    "for i, text in enumerate(tables):\n",
    "    if hasattr(text, \"metadata\") and hasattr(text.metadata, \"filename\"):\n",
    "        filename = text.metadata.filename\n",
    "    elif (\n",
    "        hasattr(text, \"metadata\")\n",
    "        and isinstance(text.metadata, dict)\n",
    "        and \"filename\" in text.metadata\n",
    "    ):\n",
    "        filename = text.metadata[\"filename\"]\n",
    "    else:\n",
    "        filename = \"NO_FILENAME\"\n",
    "\n",
    "    print(filename)\n",
    "    filenames_for_tables.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f980e62",
   "metadata": {},
   "source": [
    "### Old way - Add both vector(PGVector) and docstore to postgres for Tables\n",
    "### Langfuse cant detect the structure since it is not a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Adding tables to retriever...\")\n",
    "# table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "# summary_tables = [\n",
    "#     Document(page_content=summary, metadata={id_key: table_ids[i]})\n",
    "#     for i, summary in enumerate(table_summaries)\n",
    "# ]\n",
    "# retriever.vectorstore.add_documents(summary_tables)\n",
    "# retriever.docstore.mset(list(zip(table_ids, tables, filenames_for_tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf4866",
   "metadata": {},
   "source": [
    "### New way using Document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding tables to retriever...\")\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "\n",
    "# Create summary documents for vectorstore (these get searched)\n",
    "summary_tables = [\n",
    "    Document(\n",
    "        page_content=summary,\n",
    "        metadata={\n",
    "            id_key: table_ids[i],\n",
    "            \"doc_type\": \"table\",\n",
    "            \"content_type\": \"table_summary\",\n",
    "        },\n",
    "    )\n",
    "    for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "\n",
    "# Create full documents for docstore (these get returned)\n",
    "full_table_docs = [\n",
    "    Document(\n",
    "        page_content=table if isinstance(table, str) else str(table),\n",
    "        metadata={\n",
    "            id_key: table_ids[i],\n",
    "            \"doc_type\": \"table\",\n",
    "            \"filename\": ( ##Not needed i think but leave it first\n",
    "                filenames_for_tables[i]\n",
    "                if i < len(filenames_for_tables)\n",
    "                else f\"table_{i}\"\n",
    "            ),\n",
    "            \"content_type\": \"table_full\",\n",
    "        },\n",
    "    )\n",
    "    for i, table in enumerate(tables)\n",
    "]\n",
    "\n",
    "# Add summaries to vectorstore for searching\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "\n",
    "# Add full documents to docstore for retrieval - FIXED FORMAT\n",
    "retriever.docstore.mset(list(zip(table_ids, full_table_docs, filenames_for_tables)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32502b67",
   "metadata": {},
   "source": [
    "### Define filename to map record for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_images = []\n",
    "for i, source_filename in enumerate(image_source_filenames):\n",
    "    # Create unique filename for each image\n",
    "    image_filename = f\"{source_filename}_image_{i}\"\n",
    "    filenames_for_images.append(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e37d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_for_images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de2cfe",
   "metadata": {},
   "source": [
    "### Old way - langfuse cannot recognise\n",
    "### Add both vector(PGVector) and docstore to postgres for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437430c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Adding images to retriever...\")\n",
    "# img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "# summary_img = [\n",
    "#     Document(page_content=summary, metadata={id_key: img_ids[i]})\n",
    "#     for i, summary in enumerate(image_summaries)\n",
    "# ]\n",
    "# retriever.vectorstore.add_documents(summary_img)\n",
    "# retriever.docstore.mset(list(zip(img_ids, images, filenames_for_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac55c5",
   "metadata": {},
   "source": [
    "### New way using Document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47554b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adding images to retriever...\")\n",
    "img_ids = [str(uuid.uuid4()) for _ in filenames_for_images]\n",
    "\n",
    "# Create summary documents for vectorstore (these get searched)\n",
    "summary_img = [\n",
    "    Document(\n",
    "        page_content=summary,\n",
    "        metadata={\n",
    "            id_key: img_ids[i],\n",
    "            \"doc_type\": \"image\",\n",
    "            \"content_type\": \"image_summary\",\n",
    "        },\n",
    "    )\n",
    "    for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "\n",
    "# Create full documents for docstore (these get returned)\n",
    "full_image_docs = [\n",
    "    Document(\n",
    "        page_content=image if isinstance(image, str) else str(image),\n",
    "        metadata={\n",
    "            id_key: img_ids[i],\n",
    "            \"doc_type\": \"image\",\n",
    "            \"filename\": (  ##Not needed i think but leave it first\n",
    "                filenames_for_images[i]\n",
    "                if i < len(filenames_for_images)\n",
    "                else f\"image_{i}\"\n",
    "            ),\n",
    "            \"content_type\": \"image_full\",\n",
    "        },\n",
    "    )\n",
    "    for i, image in enumerate(images)\n",
    "]\n",
    "\n",
    "# Add summaries to vectorstore for searching\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "\n",
    "# Add full documents to docstore for retrieval - FIXED FORMAT\n",
    "retriever.docstore.mset(list(zip(img_ids, full_image_docs, filenames_for_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1a87f",
   "metadata": {},
   "source": [
    "### Check In memory store data -\n",
    "#### can check DB also `select * from public.bytestore;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the keys currently in the store\n",
    "all_doc_ids = store.yield_keys()\n",
    "\n",
    "# Loop through and fetch each document by its ID\n",
    "for doc_id in all_doc_ids:\n",
    "    docs = store.mget([doc_id])  # Returns a list with the document(s)\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    for doc in docs:\n",
    "        print(doc)  # `doc` is a Document object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535f1f61",
   "metadata": {},
   "source": [
    "### check Chroma document - 15 data - obselete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59dee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs = vectorstore.get()\n",
    "\n",
    "# index = 0\n",
    "# for doc in all_docs[\"documents\"]:\n",
    "#     print(\"index is :\", index)\n",
    "#     print(doc)\n",
    "#     index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96abdf6",
   "metadata": {},
   "source": [
    "### list top 1000 records in postgres vector DB(PG Vector)\n",
    "#### can check DB also `select * from langchain_pg_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(\" \", k=1000)  # or any number large enough\n",
    "\n",
    "for index, doc in enumerate(docs):\n",
    "    print(\"index is:\", index)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi-modal RAG setup complete!\")\n",
    "print(f\"Processed: {len(texts)} texts, {len(tables)} tables, {len(images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58b28f",
   "metadata": {},
   "source": [
    "### Not required search_kwargs unless, need to specifically retrieve top few results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.search_kwargs = {\"k\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"what are the nationalities allowed for S-pass holders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99327205",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0]\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c4f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.vectorstore.similarity_search(\n",
    "    \"what are the nationalities allowed for S-pass holders\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14cd56",
   "metadata": {},
   "source": [
    "### some sameples using similarity_search can get the doc id but .invoke cannot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the retriever is actually doing\n",
    "docs = retriever.invoke(\"what are the nationalities allowed for S-pass holders\")\n",
    "\n",
    "# The retriever first searches vector store for summaries\n",
    "query = \"what are the nationalities allowed for S-pass holders\"\n",
    "relevant_summaries = retriever.vectorstore.similarity_search(query, k=4)\n",
    "\n",
    "print(\"=== RELEVANT SUMMARIES FROM VECTOR STORE ===\")\n",
    "for summary in relevant_summaries:\n",
    "    print(f\"Summary: {summary.page_content[:100]}...\")\n",
    "    print(f\"Metadata: {summary.metadata}\")\n",
    "    doc_id = summary.metadata.get(\"doc_id\")\n",
    "    if doc_id:\n",
    "        print(f\"Will retrieve full doc with ID: {doc_id}\")\n",
    "    print()\n",
    "\n",
    "# Then retrieves full docs from byte store\n",
    "if relevant_summaries:\n",
    "    doc_ids = [\n",
    "        summary.metadata[\"doc_id\"]\n",
    "        for summary in relevant_summaries\n",
    "        if \"doc_id\" in summary.metadata\n",
    "    ]\n",
    "    print(f\"Looking up doc_ids: {doc_ids}\")\n",
    "    full_docs = retriever.docstore.mget(doc_ids)\n",
    "    print(f\"Retrieved {len(full_docs)} full documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f276f",
   "metadata": {},
   "source": [
    "### Print the formatted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb041a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c29211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from base64 import b64decode\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63fc64",
   "metadata": {},
   "source": [
    "### Define model for Query and Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Choose your preferred model\n",
    "RAG_MODEL = (\n",
    "    \"llama3.1:8b\"  # Recommended alternatives: \"llama3.2:3b\", \"mistral:7b\", \"qwen2:7b\"\n",
    ")\n",
    "VISION_MODEL = \"gemma3:12b\"  # For handling images in RAG\n",
    "\n",
    "print(f\"Using RAG model: {RAG_MODEL}\")\n",
    "print(f\"Using Vision model: {VISION_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e0377",
   "metadata": {},
   "source": [
    "### For detecting images vs text, it will be pass to LLM after vectorstore retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    \"\"\"\n",
    "    Split retrieved documents into base64-encoded images and text content\n",
    "\n",
    "    Args:\n",
    "        docs: List of retrieved documents from the vector store\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains separated 'images' and 'texts' lists\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(docs)} retrieved documents\")\n",
    "\n",
    "    b64_images = []\n",
    "    text_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        print(f\"==>> doc: {doc}\")\n",
    "        print(f\"==>> doc type: {type(doc).__name__}\")\n",
    "        # Extract content from Document object\n",
    "        # Think all is already page_content images, text, tables, should be save to remove this\n",
    "        if hasattr(doc, \"page_content\"):\n",
    "            content = doc.page_content\n",
    "            print(f\"==>> content preview: {content[:100]}...\")\n",
    "        else:\n",
    "            content = str(doc)\n",
    "            print(f\"==>> raw content: {content[:100]}...\")\n",
    "\n",
    "        # Check if document content is base64 encoded (likely an image)\n",
    "        try:\n",
    "            # Try to decode as base64\n",
    "            # decoded = b64decode(doc.page_content)\n",
    "            print(\"testing123123\")\n",
    "            clean_content = content.strip().replace('\\n', '').replace('\\r', '').replace(' ', '')\n",
    "            print(\"is same content as clean_content\", content == clean_content)\n",
    "            b64decode(clean_content)\n",
    "            # If successful, it's likely base64 encoded image data\n",
    "            if hasattr(doc, \"page_content\"):\n",
    "                doc.page_content = clean_content\n",
    "\n",
    "            # Append the document object (not just content)\n",
    "            b64_images.append(b64decode(clean_content))\n",
    "            print(f\"Found base64 image document\")\n",
    "        except Exception as e:\n",
    "            # If decoding fails, treat as text\n",
    "            text_docs.append(doc)\n",
    "            print(f\"Found text document: {doc}...\")\n",
    "\n",
    "    return {\"images\": b64_images, \"texts\": text_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3451d",
   "metadata": {},
   "source": [
    "### constuct text only, using llama3 non vision model - can look to improve the prompt perhaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5638d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_text_only(kwargs):\n",
    "    \"\"\"\n",
    "    Build prompt for text-only RAG (when no images are present)\n",
    "    Uses the main RAG model for faster processing\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    # Combine all text content\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_doc in docs_by_type[\"texts\"]:\n",
    "            # Handle both string content and Document objects\n",
    "            if hasattr(text_doc, \"page_content\"):\n",
    "                context_text += text_doc.page_content + \"\\n\\n\"\n",
    "            else:\n",
    "                context_text += str(text_doc) + \"\\n\\n\"\n",
    "\n",
    "    # Simple text-based prompt template\n",
    "    prompt_template = f\"\"\"You are a helpful assistant answering questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context_text.strip()}\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based only on the provided context\n",
    "- If the context doesn't contain relevant information, say \"I don't have enough information to answer this question based on the provided context\"\n",
    "- Be concise and accurate\n",
    "- If referencing specific data or facts, mention them clearly\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8ee65",
   "metadata": {},
   "source": [
    "### Construct prompt using a vision model (Gemma) for images and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_vision(kwargs):\n",
    "    \"\"\"\n",
    "    Build prompt for multi-modal RAG (when images are present)\n",
    "    Uses the vision model to handle both text and images\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    # Combine text content\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_doc in docs_by_type[\"texts\"]:\n",
    "            if hasattr(text_doc, \"page_content\"):\n",
    "                context_text += text_doc.page_content + \"\\n\\n\"\n",
    "            else:\n",
    "                context_text += str(text_doc) + \"\\n\\n\"\n",
    "\n",
    "    # Base prompt text\n",
    "    prompt_text = f\"\"\"You are a helpful assistant answering questions based on the provided context, which includes both text and images.\n",
    "\n",
    "Text Context:\n",
    "{context_text.strip()}\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based on both the text context and the images provided\n",
    "- If analyzing images, describe what you see that's relevant to the question\n",
    "- Be specific about information from images (charts, diagrams, etc.)\n",
    "- If the context doesn't contain relevant information, say so clearly\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Build content list starting with text\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "    # Add images if present\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        print(f\"Adding {len(docs_by_type['images'])} images to prompt\")\n",
    "        for i, image_b64 in enumerate(docs_by_type[\"images\"]):\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([HumanMessage(content=prompt_content)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b05c7",
   "metadata": {},
   "source": [
    "### Dynamically choose between Llama and Gemma, actually dont need it can use Gemma for all but this will improve the speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_and_prompt(kwargs):\n",
    "    \"\"\"\n",
    "    Dynamically choose between text-only and vision model based on content\n",
    "    \"\"\"\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    print(f\"==>> docs_by_type: {docs_by_type}\")\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        # Use vision model for multi-modal content\n",
    "        print(\"Using vision model for multi-modal RAG\")\n",
    "        prompt = build_prompt_with_vision(kwargs)\n",
    "        model = ChatOllama(\n",
    "            model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "    else:\n",
    "        # Use text model for text-only content (faster)\n",
    "        print(\"Using text model for text-only RAG\")\n",
    "        prompt = build_prompt_text_only(kwargs)\n",
    "        model = ChatOllama(\n",
    "            model=RAG_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "\n",
    "    return prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f2ea7",
   "metadata": {},
   "source": [
    "### Setting up 4 types of chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ae489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG Chain\n",
    "print(\"Setting up RAG chain...\")\n",
    "chain = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(choose_model_and_prompt)\n",
    "\n",
    "# Alternative: Simple chain that always uses text model (faster but no vision)\n",
    "simple_text_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt_text_only)\n",
    "    | ChatOllama(model=RAG_MODEL, temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain with sources (returns both context and response)\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(response=RunnableLambda(choose_model_and_prompt))\n",
    "\n",
    "# Alternative: Always use vision model (slower but handles all content types)\n",
    "vision_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt_with_vision)\n",
    "    | ChatOllama(model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "vision_chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt_with_vision)\n",
    "        | ChatOllama(\n",
    "            model=VISION_MODEL, temperature=0.1, base_url=\"http://localhost:11434\"\n",
    "        )\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c44674",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11df83",
   "metadata": {},
   "source": [
    "### testing chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0374d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple question\n",
    "response = chain.invoke(\"How is fin created?\", config={\"callbacks\": [langfuse_handler]})\n",
    "print(f\"==>> response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sources\n",
    "# response = vision_chain_with_sources.invoke(\n",
    "#     \"if a company has 2 CSN account, would a levy default bl applied to both CSN if the company failed to pay the levy for one of the CSN, is there a table base on what type of CSN they are holding?\",\n",
    "#     config={\"callbacks\": [langfuse_handler]},\n",
    "# )\n",
    "response = chain_with_sources.invoke(\n",
    "    \"Is security bond (SB) needed for all work permit (WP) holders? How about for S-Pass and Employment Pass (EP)?\",\n",
    "    config={\"callbacks\": [langfuse_handler]},\n",
    ")\n",
    "print(f\"==>> response: {response}\")\n",
    "print(\"Response:\", response[\"response\"])\n",
    "print(\"Context used:\", response[\"context\"])\n",
    "for text in response[\"context\"][\"texts\"]:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "for image in response[\"context\"][\"images\"]:\n",
    "    display_base64_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
